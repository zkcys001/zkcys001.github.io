<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kecheng Zheng</title>
  
  <meta name="author" content="Kecheng Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/zkc.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kecheng Zheng</name>
              </p>
              <p> Kecheng Zheng is currently a researcher at Ant Research working with <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>. My research focuses on computer vision and deep learning, particularly on multi-modal learning.
              </p>
              <p>
                 From Nov. 2018 to Jan. 2019, I was under the supervision of Wei Wei who is a research scientist in GOOGLE cloud.
                </p>
              <p>
                 From Sep. 2019 to May. 2020, I was an intern at JD AI Lab, working with Wu Liu.
                </p>
              <p>
                 From Jul. 2020 to Jan. 2021, I worked at Intelligent Multimedia Group (IMG) in MSRA as a research intern, under the supervision of Cuiling Lan.
              </p>
              <p>
                 From Mar. 2022 to Jul. 2022, I worked at Ant Research as a research intern working with Deli Zhao.
              </p>
<!--              <p>-->
<!--                <font color="red"><strong>-->
<!--              <strong>[08/2023] </strong> Looking for self-motivated interns in multi-modal learning in Ant Research. Please drop your CV to me if interested.</strong></font>-->
<!--              </p>-->
              <p style="text-align:center">
                <a href="mailto:zkechengzk@gmail.com">Email: zkechengzk@gmail.com</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com.hk/citations?user=hMDQifQAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
<!--                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp-->
                <a href="https://github.com/zkcys001/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zkc.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
<!--               <p><li> <font color="red"><strong>[10/2023] Looking for self-motivated interns in multi-modal learning. Please drop your CV to me if interested.</strong></font></p> -->
              <p><li> <font color="red"><strong>[11/2024] Three papers accepted by NeurIPS 2024~</strong></font></p>
              <p><li><strong>[06/2024] </strong> Four papers accepted by ECCV 2024~</p>
              <p><li><strong>[03/2024] </strong> Two papers accepted by CVPR 2024</p>
              <p><li><strong>[10/2023] </strong> Two papers accepted by NeurIPS 2023.</p>
              <p><li><strong>[08/2023] </strong> Two papers accepted by ICCV 2023.</p>
              <p><li><strong>[04/2023] </strong> Two papers accepted by ICML 2023.</p>
              <p><li><strong>[02/2023] </strong> Two papers accepted by CVPR 2023.</p>
<!--              <p><li><strong>[09/2022] </strong> Two paper accepted by NeurIPS 2022.</p>
<!--              <p><li><strong>[05/2022] </strong> One paper accepted by ICML 2022.</p> --> -->
<!--              <p><li><strong>[03/2022] </strong> Three paper accepted by CVPR 2022.</p>-->
<!--              <p><li><strong>[12/2021] </strong> Two paper accepted by AAAI 2022.</p>-->
<!--              <p><li><strong>[12/2021] </strong> Awarded the National Scholarship for Doctoral Students (¥30000).</p>-->
<!--              <p><li><strong>[07/2021] </strong> One paper accepted by ACM MM 2021.</p>-->
<!--              <p><li><strong>[03/2021] </strong> Two papers accepted by CVPR 2021 (one paper as an oral presentation).</p>-->
<!--              <p><li><strong>[12/2020] </strong> One paper accepted by AAAI 2021.</p>-->
<!--              <p><li><strong>[11/2020] </strong> Awarded the Suzhou Industrial Park Scholarship (¥3000).</p>-->
<!--              <p><li><strong>[07/2020] </strong> One paper accepted by ACM MM 2020.</p>-->
<!--              <p><li><strong>[03/2020] </strong> One paper accepted by ICME 2020.</p>-->
<!--              <p><li><strong>[09/2019] </strong> One paper accepted by NeurIPS 2019.</p>-->
<!--              <p><li><strong>[11/2018] </strong> Awarded the Huawei Scholarship (¥8000).</p>-->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, representation learning and multi-modal learning.
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/codef.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                CoDeF: Content Deformation Fields for Temporally Consistent Video Processing
              </div>
              <div class="author">
                <a href="https://ken-ouyang.github.io" target="_blank">Hao Ouyang*</a>,
                <a href="https://github.com/qiuyu96" target="_blank">Qiuyu Wang*</a>,
                <a href="https://henry123-boy.github.io/" target="_blank">Yuxi Xiao*</a>,
                <a href="" target="_blank">Qingyan Bai</a>,
                <a href="" target="_blank">Juntao Zhang</a>,
                <strong>Kecheng Zheng</strong>,
                <a href="" target="_blank">Xiaowei Zhou</a>,
                <a href="" target="_blank">Qifeng Chen</a>,
                <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>
              </div>
              <em><font color="#663399"><strong>Arxiv, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://arxiv.org/abs/2308.07926">Arxiv</a> /
                <a href="https://qiuyu96.github.io/CoDeF/">Project</a> /
                <a href="https://github.com/qiuyu96/CoDeF">Code</a> <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiuyu96/CoDeF?style=social">
              </div>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/carver.jpg">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase
              </div>
              <div class="author">
                <a href="https://github.com/qiuyu96" target="_blank">Qiuyu Wang*</a>,
                <a href="https://vivianszf.github.io/" target="_blank">Zifan Shi</a>,
                <strong>Kecheng Zheng</strong>,
                <a href="https://justimyhxu.github.io/" target="_blank">Yinghao Xu</a>,
                <a href="https://pengsida.net" target="_blank">Sida Peng</a>,
                <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>
              </div>
              <em><font color="#663399"><strong>NeurIPS, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://arxiv.org/pdf/2306.12423.pdf" target="_blank">ArXiv</a> /
                <a href="https://github.com/qiuyu96/carver" target="_blank">Code</a> <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiuyu96/Carver?style=social">
              </div>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/cones2.jpg">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Cones 2: Customizable Image Synthesis with Multiple Subjects
              </div>
              <div class="author">
                Zhiheng Liu*,
                Yifei Zhang*,
                <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>,
                <strong>Kecheng Zheng</strong>,
                Kai Zhu,
                Ruili Feng,
                <a href="https://scholar.google.com/citations?user=8zksQb4AAAAJ" target="_blank">Yu Liu</a>,
                <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,
                <a href="https://scholar.google.com/citations?user=64zxhRUAAAAJ" target="_blank">Jingren Zhou</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7rTHNcAAAAJ&view_op=list_works&sortby=pubdate">Yang Cao</a>
              </div>
              <em><font color="#663399"><strong>NeurIPS, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://arxiv.org/pdf/2305.19327.pdf" target="_blank">ArXiv</a> /
                <a href="https://cones-page.github.io/" target="_blank">Project</a> /
                <a href="https://github.com/damo-vilab/Cones-V2" target="_blank">Code</a> <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/damo-vilab/Cones-V2?style=social">
              </div>
            </td>
          </tr>

           <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/cones.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Cones: Concept Neurons in Diffusion Models for Customized Generation.
              </div>
              <div class="author">
                Zhiheng Liu*,
                Ruili Feng*,
                Kai Zhu,
                Yifei Zhang,
                <strong>Kecheng Zheng</strong>,
                <a href="https://scholar.google.com/citations?user=8zksQb4AAAAJ" target="_blank">Yu Liu</a>,
                <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,
                <a href="https://scholar.google.com/citations?user=64zxhRUAAAAJ" target="_blank">Jingren Zhou</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7rTHNcAAAAJ&view_op=list_works&sortby=pubdate">Yang Cao</a>
              </div>
              <em><font color="#663399"><strong>ICML, 2023</strong></font></em></em> &nbsp <font color="red"><strong>Oral Presentation!</strong></font>
              <br>
              <div class="material">
                <a href="https://arxiv.org/pdf/2303.05125.pdf" target="_blank">ArXiv</a> /
                <a href="https://github.com/damo-vilab/Cones" target="_blank">Code</a> <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/damo-vilab/Cones?style=social">
              </div>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/rleg.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                RLEG: Vision-Language Representation Learning with Diffusion-based Embedding Generation.
              </div>
              <div class="author">
                Liming Zhao,
                <strong>Kecheng Zheng</strong>,
                Yun Zheng,
                <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,
                <a href="https://scholar.google.com/citations?user=64zxhRUAAAAJ" target="_blank">Jingren Zhou</a>,
              </div>
              <em><font color="#663399"><strong>ICML, 2023</strong></font></em></em> &nbsp
              <br>
              <div class="material">
                <a href="https://openreview.net/pdf?id=zBShO1Vmf0" target="_blank">ICML</a>
              </div>
            </td>
          </tr>


          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/pathcl.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Self-Organizing Pathway Expansion for Non-Exemplar Class-Incremental Learning
              </div>
              <div class="author">
                Kai Zhu*,
                <strong>Kecheng Zheng*</strong>,
                Ruili Feng,
                <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7rTHNcAAAAJ&view_op=list_works&sortby=pubdate">Yang Cao</a>,
                <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>
              </div>
              <em><font color="#663399"><strong>ICCV, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="" target="_blank">Arvix(coming soon)</a> /
                <a href="" target="_blank">Code(coming soon)</a>
              </div>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/mclip.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models
              </div>
              <div class="author">
                <strong>Kecheng Zheng*</strong>,
                Wei Wu*,
                Ruili Feng
                Kai Zhu,
                Jiawei Liu,
                <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,
                <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>,
                <a href="" target="_blank">Wei Chen</a>,
                <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>
              </div>
              <em><font color="#663399"><strong>ICCV, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://arxiv.org/pdf/2307.15049" target="_blank">Arvix</a> /
                <a href="https://wuw2019.github.io/R-AMT/" target="_blank">Project</a> /
                <a href="https://github.com/wuw2019/R-AMT" target="_blank">Code</a><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/wuw2019/R-AMT?style=social">
              </div>
            </td>
          </tr>



          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/neuraldependency.jpg">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Neural Dependencies Emerging from Learning Massive Categories
              </div>
              <div class="author">
                Ruili Feng,
                <strong>Kecheng Zheng</strong>,
                Kai Zhu,
                <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>,
                Jian Zhao,
                <a href="https://scholar.google.com/citations?user=lHb5gzoAAAAJ" target="_blank">Yukun Huang</a>,
                <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,
                <a href="https://scholar.google.com/citations?user=64zxhRUAAAAJ" target="_blank">Jingren Zhou</a>,
                <a href="http://people.eecs.berkeley.edu/~jordan/" target="_blank">Michael Jordan</a>,
                <a href="https://en.auto.ustc.edu.cn/2021/0616/c26828a513174/page.htm" target="_blank">Zheng-Jun Zha</a>
              </div>
              <em><font color="#663399"><strong>CVPR, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://arxiv.org/pdf/2211.12339.pdf" target="_blank">arvix</a>
              </div>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/uood.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection
              </div>
              <div class="author">
                Fan Lu,
                Kai Zhu,
                Wei Zhai,
                <strong>Kecheng Zheng</strong>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7rTHNcAAAAJ&view_op=list_works&sortby=pubdate">Yang Cao</a>
              </div>
              <em><font color="#663399"><strong>CVPR, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://arxiv.org/pdf/2303.10449" target="_blank">arvix</a> /
                <a href="https://github.com/LuFan31/ET-OOD" target="_blank">Code</a><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LuFan31/ET-OOD?style=social">
              </div>
            </td>
          </tr>



          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/rank.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Rank Diminishing in Deep Neural Networks
              </div>
              <div class="author">
                Ruili Feng,
                <strong>Kecheng Zheng</strong>,
                Yukun Huang,
                <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,
                <a href="https://scholar.google.com/citations?user=yxUduqMAAAAJ&hl=en" target="_blank">Michael I. Jordan</a>,
                <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>
              </div>
              <em><font color="#663399"><strong>NeurIPS, 2022</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://openreview.net/pdf?id=AJ_flTkNFhP" target="_blank">NeurIPS</a> /
                <a href="https://github.com/hyk1996/Rank-Diminishing-in-Deep-Neural-Networks" target="_blank">Code</a> <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/hyk1996/Rank-Diminishing-in-Deep-Neural-Networks?style=social">
              </div>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img style="width: 100%; max-height: 100px; object-fit: cover;" src="./images/uiirc.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="title">
                Uncertainty-Aware Hierarchical Refinement for Incremental Implicitly-Refined Classification
              </div>
              <div class="author">
                Jian Yang,
                Kai Zhu,
                <strong>Kecheng Zheng</strong>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7rTHNcAAAAJ&view_op=list_works&sortby=pubdate">Yang Cao</a>
              </div>
              <em><font color="#663399"><strong>NeurIPS, 2022</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <div class="material">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/65a723bf7d8dad838c09178270d30e80-Paper-Conference.pdf" target="_blank">NeurIPS</a>
              </div>
            </td>
          </tr>
<!--          <tr onmouseout="car_stop()" onmouseover="car_start()">-->
<!--            <td style="padding:20px;width:15%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='car'>-->
<!--                <img src='images/carver.jpg' width="160">-->
<!--                </div>-->
<!--                <img src='images/carver.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function car_start() {-->
<!--                  document.getElementById('cloth').style.opacity = "1";-->
<!--                }-->

<!--                function car_stop() {-->
<!--                  document.getElementById('cloth').style.opacity = "0";-->
<!--                }-->
<!--                unprocessing_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="">-->
<!--                <papertitle>Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--                <a href="https://github.com/qiuyu96/" target="_blank">Qiuyu Wang</a>,-->
<!--                <a href="https://vivianszf.github.io/" target="_blank">Zifan Shi</a>,-->
<!--                <strong>Kecheng Zheng</strong>,-->
<!--                <a href="https://justimyhxu.github.io/" target="_blank">Yinghao Xu</a>,-->
<!--                <a href="https://pengsida.net" target="_blank">Sida Peng</a>,-->
<!--                <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>-->

<!--              <br>-->
<!--              <em><font color="#663399"><strong>arXiv, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/pdf/2306.12423.pdf" target="_blank">ArXiv</a> /-->
<!--              <a href="https://github.com/qiuyu96/carver" target="_blank">Code</a>-->
<!--              <p></p>-->
<!--              <p>Overview of our modularized pipeline for 3D-aware image synthesis, which modularizes the generation process in a universal way. Each module can be improved independently, facilitating algorithm development. Note that the discriminator is omitted for simplicity.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="cones2_stop()" onmouseover="cones2_start()">-->
<!--            <td style="padding:20px;width:15%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='cones2'>-->
<!--                <img src='images/cones2.jpg' width="160">-->
<!--                </div>-->
<!--                <img src='images/cones2.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function cones2_start() {-->
<!--                  document.getElementById('cloth').style.opacity = "1";-->
<!--                }-->

<!--                function cones2_stop() {-->
<!--                  document.getElementById('cloth').style.opacity = "0";-->
<!--                }-->
<!--                unprocessing_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="">-->
<!--                <papertitle>Cones 2: Customizable Image Synthesis with Multiple Subjects</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              Zhiheng Liu*,-->
<!--              Yifei Zhang*,-->
<!--              <a href="https://shenyujun.github.io/" target="_blank">Yujun Shen</a>-->
<!--              <strong>Kecheng Zheng</strong>,-->
<!--              Kai Zhu,-->
<!--              Ruili Feng,-->
<!--              <a href="https://scholar.google.com/citations?user=8zksQb4AAAAJ" target="_blank">Yu Liu</a>,-->
<!--              <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=64zxhRUAAAAJ" target="_blank">Jingren Zhou</a>,-->
<!--              <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7rTHNcAAAAJ&view_op=list_works&sortby=pubdate">Yang Cao</a>-->

<!--              <br>-->
<!--              <em><font color="#663399"><strong>arXiv, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/pdf/2305.19327.pdf" target="_blank">ArXiv</a> /-->
<!--              <a href="https://cones-page.github.io/" target="_blank">Project</a> /-->
<!--              <a href="https://github.com/damo-vilab/Cones-V2" target="_blank">Code</a>-->
<!--              <p></p>-->
<!--              <p>Cones 2 uses a simple yet effective representation to register a subject. The storage space required for each subject is approximately 5 KB. Moreover, Cones 2 allows for the flexible composition of various subjects without any model tuning.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="nd_stop()" onmouseover="nd_start()">-->
<!--            <td style="padding:20px;width:15%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='nd'>-->
<!--                <img src='images/neuraldependency.jpg' width="160">-->
<!--                </div>-->
<!--                <img src='images/neuraldependency.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function nd_start() {-->
<!--                  document.getElementById('cloth').style.opacity = "1";-->
<!--                }-->

<!--                function nd_stop() {-->
<!--                  document.getElementById('cloth').style.opacity = "0";-->
<!--                }-->
<!--                unprocessing_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="">-->
<!--                <papertitle> Neural Dependencies Emerging from Learning Massive Categories</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              Ruili Feng,-->
<!--              <strong>Kecheng Zheng</strong>,-->
<!--              Kai Zhu,-->
<!--              <span class="me">Yujun Shen</span>,-->
<!--              Jian Zhao,-->
<!--              <a href="https://scholar.google.com/citations?user=lHb5gzoAAAAJ" target="_blank">Yukun Huang</a>,-->
<!--              <a href="https://sites.google.com/site/zhaodeli/" target="_blank">Deli Zhao</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=64zxhRUAAAAJ" target="_blank">Jingren Zhou</a>,-->
<!--              <a href="http://people.eecs.berkeley.edu/~jordan/" target="_blank">Michael Jordan</a>,-->
<!--              <a href="https://en.auto.ustc.edu.cn/2021/0616/c26828a513174/page.htm" target="_blank">Zheng-Jun Zha</a>-->

<!--              <br>-->
<!--              <em><font color="#663399"><strong>CVPR, 2023</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/pdf/2211.12339.pdf" target="_blank">CVPR</a>-->
<!--              <p></p>-->
<!--            </td>-->
<!--          </tr>-->

          <tr onmouseout="pre_stop()" onmouseover="pre_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pre'>
                <img src='images/pre.png' width="160">
                </div>
                <img src='images/pre.png' width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Unleashing the Potential of Unsupervised Pre-Training with Intra-Identity Regularization for Person Re-Identification</papertitle>
              </a>
              <br>
              <a href=>Zizheng Yang</a>,
              <a href="http://home.ustc.edu.cn/~jinxustc/">Xin Jin</a>,
              <strong>Kecheng Zheng</strong>,
              <a href=>Feng Zhao</a>

              <br>
              <em><font color="#663399"><strong>CVPR, 2022</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2112.00317">ArXiv</a> /
              <a href="https://github.com/Frost-Yang-99/UP-ReID">Code</a> /
              <a href="data/pre.bib">bibtex</a>
              <p></p>
              <p>We design an Unsupervised Pre-training framework for ReID based on the contrastive learning (CL) pipeline, dubbed UP-ReID.</p>
            </td>
          </tr>

          <tr onmouseout="cloth_stop()" onmouseover="cloth_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cloth'>
                <img src='images/cloth.jpg' width="160">
                </div>
                <img src='images/cloth.jpg' width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Cloth-Changing Person Re-identification from A Single Image with Gait Prediction and Regularization</papertitle>
              </a>
              <br>
              <a href="http://home.ustc.edu.cn/~jinxustc/">Xin Jin</a>,
              <a href="http://home.ustc.edu.cn/~hetianyu/">Tianyu He</a>,
              <strong>Kecheng Zheng</strong>,
              <a href=>Zhiheng Ying</a>,
              <a href=>Xu Shen</a>,
              <a href=>Zhen Huang </a>,
              <a href=>Ruoyu Feng </a>,
              <a href=>Jianqiang Huang </a>,
              <a href=>Xian-Sheng Hua </a>,
              <a href=>Zhibo Chen</a>

              <br>
              <em><font color="#663399"><strong>CVPR, 2022</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2103.15537.pdf">ArXiv</a> /
              <a href="https://github.com/jinx-USTC/GI-ReID">Code</a> /
              <a href="data/cloth.bib">bibtex</a>
              <p></p>
              <p>We focus on handling well the CC-ReID problem under a more challenging setting, i.e., just from a single image, which enables high-efficiency and latency-free pedestrian identify for real-time surveillance applications.</p>
            </td>
          </tr>

          <tr onmouseout="CFD_stop()" onmouseover="CFD_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CFD'>
                <img src='images/CFD.jpg' width="160">
                </div>
                <img src='images/CFD.jpg' width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('CFD').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('CFD').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Calibrated Feature Decomposition for Generalizable Person Re-Identification</papertitle>
              </a>
              <br>
              <strong>Kecheng Zheng</strong>,
              <a href=>Jiawei Liu</a>,
              <a href=>Wei Wu</a>,
              <a href=>Liang Li</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>


              <br>
              <em><font color="#663399"><strong>Arxiv, 2022</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2111.13945.pdf">ArXiv</a> /
              <a href="https://github.com/zkcys001/CFD">Code</a> /
              <a href="data/CFD.bib">bibtex</a>
              <p></p>
              <p>We propose a simple yet effective Calibrated Feature Decomposition (CFD) module that focuses on improving the generalization capacity for person re-identification through a more judicious feature decomposition and reinforcement strategy.</p>
            </td>
          </tr>

          <tr onmouseout="GDN_stop()" onmouseover="GDN_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='GDN'>
                <img src='images/GDN.jpg' width="160">
                </div>
                <img src='images/GDN.jpg' width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('GDN').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('GDN').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-Identification</papertitle>
              </a>
              <br>
              <a href=>Jiawei Liu</a>,
              <a href=>Zhipeng Huang</a>,
              <strong>Kecheng Zheng</strong>,
              <a href=>Liang Li</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>


              <br>
              <em><font color="#663399"><strong>AAAI, 2022</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2203.01723">Arxiv</a>
              <p></p>
              <p>We propose a novel Debiased Batch Normalization via Gaussian Process approach (GDNorm) for generalizable person re-identification, which models the feature statistic estimation from BN layers as a dynamically self-refining Gaussian process to alleviate the bias to unseen domain for improving the generalization.</p>
            </td>
          </tr>

          <tr onmouseout="MID_stop()" onmouseover="MID_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='MID'>
                <img src='images/MID.jpg' width="160">
                </div>
                <img src='images/MID.jpg' width="160">
              </div>
              <script type="text/javascript">
                function MID_start() {
                  document.getElementById('MID').style.opacity = "1";
                }

                function MID_stop() {
                  document.getElementById('MID').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-Identification</papertitle>
              </a>
              <br>
              <a href=>Zhipeng Huang</a>,
              <a href=>Jiawei Liu</a>,
              <strong>Kecheng Zheng</strong>,
              <a href=>Liang Li</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>


              <br>
              <em><font color="#663399"><strong>AAAI, 2022</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2203.01735">arxiv</a>
              <p></p>
              <p>We propose a novel modality-adaptive mixup and invariant decomposition (MID) approach for RGB-infrared person re-identification towards learning modality-invariant and discriminative representations.</p>
            </td>
          </tr>

          <tr onmouseout="pos_stop()" onmouseover="pos_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pos'>
                <img src='images/pos.jpg' width="160">
                </div>
                <img src='images/pos.jpg' width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('pos').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('pos').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification</papertitle>
              </a>
              <br>
              <strong>Kecheng Zheng</strong>,
              <a href=>Cuiling Lan</a>,
              <a href=>Jiawei Liu</a>,
              <a href=>Wenjun Zeng</a>,
              <a href=>Zhizheng Zhang</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>


              <br>
              <em><font color="#663399"><strong>ACM MM, 2021</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2108.00139.pdf">ArXiv</a> /
<!--              <a href="https://github.com/zkcys001/UDAStrongBaseline">Code</a> /-->
              <a href="data/pos2021.bib">bibtex</a>
              <p></p>
              <p>We propose a network named Pose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the pose information is exploited to regularize the learning of semantics aligned features but is discarded in testing.</p>
            </td>
          </tr>

          <tr onmouseout="GLT_stop()" onmouseover="GLT_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='GLT'>
                <img src='images/GLT.jpg' width="160">
                </div>
                <img src='images/GLT.jpg' width="160">
              </div>
              <script type="text/javascript">
                function GLT_start() {
                  document.getElementById('GLT').style.opacity = "1";
                }

                function GLT_stop() {
                  document.getElementById('GLT').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Group-aware Label Transfer for Domain Adaptive Person Re-identification</papertitle>
              </a>
              <br>
              <strong>Kecheng Zheng</strong>,
              <a href="http://liuwu.weebly.com/">Wu Liu</a>,
              <a href="https://lingxiao-he.github.io/">Lingxiao He</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>,
              <a href="https://taomei.me/?s=Mei">Tao Mei</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>

              <br>
              <em><font color="#663399"><strong>CVPR, 2021</strong></font></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2103.12366.pdf">ArXiv</a> /
              <a href="https://github.com/zkcys001/UDAStrongBaseline">Code</a> /
              <a href="data/GLT2021.bib">bibtex</a>
              <p></p>
              <p>We propose a Group-aware Label Transfer (GLT) algorithm, which enables the online interaction and mutual promotion of pseudo-label prediction and representation learning.</p>
            </td>
          </tr>

          <tr onmouseout="SCTL_stop()" onmouseover="SCTL_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='SCTL'>
                <img src='images/STCL.jpg' width="160">
                </div>
                <img src='images/STCL.jpg' width="160">
              </div>
              <script type="text/javascript">
                function SCTL_start() {
                  document.getElementById('unprocessing_image').style.opacity = "1";
                }

                function SCTL_stop() {
                  document.getElementById('unprocessing_image').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Spatial-Temporal Correlation and Topology Learning for Person Re-Identification in Videos</papertitle>
              </a>
              <br>
              <a href=>Jiawei Liu</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>,
              <a href=>Wei Wu</a>,
              <strong>Kecheng Zheng</strong>,
              <a href=>Qibin Sun</a>

              <br>
              <em><font color="#663399"><strong>CVPR, 2021</strong></font></em></em> &nbsp <font color="red"><strong>Oral Presentation!</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2104.08241.pdf">ArXiv</a> /
<!--              <a href="">code(coming soon)</a> /-->
              <a href="data/SCTL2021.bib">bibtex</a>
              <p></p>
              <p>We propose a novel Spatial-Temporal Correlation and Topology Learning framework (CTL) to pursue discriminative and robust representation by modeling cross-scale spatial-temporal correlation.</p>
            </td>
          </tr>

          <tr onmouseout="disen_stop()" onmouseover="disen_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='disen'>
                <img src='images/disen.jpg' width="160">
                </div>
                <img src='images/disen.jpg' width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('unprocessing_image').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('unprocessing_image').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Disentanglement-based Cross-Domain Feature Augmentation for Effective Unsupervised Domain Adaptive Person Re-identification</papertitle>
              </a>
              <br>
              <a href=>Zhizheng Zhang </a>,
              <a href=>Cuiling Lan</a>,
              <a href=>Wenjun Zeng</a>,
              <a href=>Quanzeng You </a>,
              <a href=>Zicheng Liu </a>,
              <strong>Kecheng Zheng</strong>,
              <a href=>Zhibo Chen</a>

              <br>
              <em>ArXiv</em>, 2021 &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2103.13917.pdf">ArXiv</a> /
              <a href="">code(coming soon)</a> /
              <a href="data/disen.bib">bibtex</a>
              <p></p>
              <p>We propose a Disentanglement-based Cross-Domain Feature Augmentation (DCDFA) strategy, where the augmented features characterize well the target and source domain data distributions while inheriting reliable identity labels.</p>
            </td>
          </tr>

          <tr onmouseout="unc_stop()" onmouseover="unc_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='unc'>
                <img src='images/unc.jpg' width="160">
                </div>
                <img src='images/unc.jpg' width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('unc').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('unc').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification</papertitle>
              </a>
              <br>
              <strong>Kecheng Zheng</strong>,
              <a href=>Cuiling Lan</a>,
              <a href=>Wenjun Zeng</a>,
              <a href=>Zhizheng Zhang</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>


              <br>
              <em><font color="#663399"><strong>AAAI, 2021</strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2012.08733.pdf">ArXiv</a> /
              <a href="https://github.com/zkcys001/UDAStrongBaseline">Code</a> /
              <a href="data/unc2021.bib">bibtex</a>
              <p></p>
              <p>We propose to estimate and exploit the  credibility of the assigned pseudo-label of each sample to alleviate the influence of noisy labels.</p>
            </td>
          </tr>

          <tr onmouseout="vtm_stop()" onmouseover="vtm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vtm'>
                <img src='images/vtm.jpg' width="160">
                </div>
                <img src='images/vtm.jpg' width="160">
              </div>
              <script type="text/javascript">
                function vtm_start() {
                  document.getElementById('vtm').style.opacity = "1";
                }

                function vtm_stop() {
                  document.getElementById('vtm').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval</papertitle>
              </a>
              <br>
              <a href="">Rui Zhao*</a>,
              <strong>Kecheng Zheng*</strong>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>,
              <a href="">Hongtao Xie</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>*Equal Contribution

              <br>
              <em>ArXiv</em>, 2020 &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2103.15686.pdf">ArXiv</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>We propose a novel memory enhanced embedding learning (MEEL) method for video-text retrieval.</p>
            </td>
          </tr>

          <tr onmouseout="htp_stop()" onmouseover="htp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='htp'>
                <img src='images/htp.jpg' width="160">
                </div>
                <img src='images/htp.jpg' width="160">
              </div>
              <script type="text/javascript">
                function htp_start() {
                  document.getElementById('abs').style.opacity = "1";
                }

                function htp_stop() {
                  document.getElementById('abs').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Hierarchical Gumbel Attention Network for Text-based Person Search</papertitle>
              </a>
              <br>

              <strong>Kecheng Zheng</strong>,
              <a href="http://liuwu.weebly.com/">Wu Liu</a>,
              <a href="https://lingxiao-he.github.io/">Jiawei Liu</a>,
              <a href="https://taomei.me/?s=Mei">Tao Mei</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>

              <br>
              <em><font color="#663399"><strong>ACM MM, 2020 </strong></font></em></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://www.researchgate.net/profile/Kecheng-Zheng/publication/346192190_Hierarchical_Gumbel_Attention_Network_for_Text-based_Person_Search/links/604727714585154e8c878f1b/Hierarchical-Gumbel-Attention-Network-for-Text-based-Person-Search.pdf?_sg%5B0%5D=Hq7hZQonooMDf7A82hM5BbfE45nZW5iSJqoeJDbsINa_q1qJclyw8r_w_5hObWoC14OkXdqwVSiREJSRnDwcNw.NUMi9TJcGDMYIit1hcRJsyvxIdrfrd9oXE7NDY90zwUoISLE1jCcuhm5nXuaUM4tdS7xlkBeIPGOcUlLdQz7EA&_sg%5B1%5D=G57TCmgqt6GIYLNqoyTQliTRewmuo16Xkz5Pk4C2XMFeeQnEaqpf6EBwKrc0TmwhyvsBqBDX2N6XB0Xq7pdHkui6gaEzxO2IuD_U-vfoSKUJ.NUMi9TJcGDMYIit1hcRJsyvxIdrfrd9oXE7NDY90zwUoISLE1jCcuhm5nXuaUM4tdS7xlkBeIPGOcUlLdQz7EA&_iepl=">Acm</a> /
              <a href="data/htp2019.bib">bibtex</a>
              <p></p>
              <p>We propose a novel hierarchical Gumbel attention network for text-based person search via Gumbel top-k re-parameterization algorithm.</p>
            </td>
          </tr>

          <tr onmouseout="abs_stop()" onmouseover="abs_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='abs'>
                <img src='images/abs.jpg' width="160">
                </div>
                <img src='images/abs.jpg' width="160">
              </div>
              <script type="text/javascript">
                function abs_start() {
                  document.getElementById('abs').style.opacity = "1";
                }

                function abs_stop() {
                  document.getElementById('abs').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Abstract Reasoning with Distracting Features</papertitle>
              </a>
              <br>
              <strong>Kecheng Zheng</strong>,
              <a href="http://www.weiwei.one/">Wei Wei</a>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>


              <br>
               <em><font color="#663399"><strong>NeurIPS, 2019 </strong></font></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/1912.00569">ArXiv</a> /
              <a href="https://github.com/zkcys001/distracting_feature">Code</a> /
              <a href="data/abs2019.bib">bibtex</a>
              <p></p>
              <p>We first illustrate that one of the main challenges in such a reasoning task is the presence of distracting features, which requires the learning algorithm to leverage counterevidence
and to reject any of the false hypotheses in order to learn the true patterns.</p>
            </td>
          </tr>

          <tr onmouseout="vt_stop()" onmouseover="vt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vt'>
                <img src='images/vt.jpg' width="160">
                </div>
                <img src='images/vt.jpg' width="160">
              </div>
              <script type="text/javascript">
                function vt_start() {
                  document.getElementById('vt').style.opacity = "1";
                }

                function vt_stop() {
                  document.getElementById('vt').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>STACKED CONVOLUTIONAL DEEP ENCODING NETWORK FOR VIDEO-TEXT RETRIEVAL</papertitle>
              </a>
              <br>
              <a href="">Rui Zhao*</a>,
              <strong>Kecheng Zheng*</strong>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>
              <br>*Equal Contribution

              <br>
              <em><font color="#663399"><strong>ICME, 2020 </strong></font></em> &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2004.04959.pdf">ArXiv</a> /
              <a href="data/vt.bib">bibtex</a>
              <p></p>
              <p>We propose a stacked convolutional deep encoding network for video-text retrieval task, which considers to simultaneously encode long-range and short-range dependency in the videos and texts.</p>
            </td>
          </tr>

          <tr onmouseout="layout_stop()" onmouseover="layout_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='layout'>
                <img src='images/layout.jpg' width="160">
                </div>
                <img src='images/layout.jpg' width="160">
              </div>
              <script type="text/javascript">
                function layout_stop() {
                  document.getElementById('layout').style.opacity = "1";
                }

                function layout_start() {
                  document.getElementById('layout').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>LA-Net: Layout-Aware Dense Network for Monocular Depth Estimation</papertitle>
              </a>
              <br>

              <strong>Kecheng Zheng</strong>,
              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7rTHNcAAAAJ&view_op=list_works&sortby=pubdate">Yang Cao</a>,
              <a href="">Xuejin Chen</a>,
              <a href="">Feng Wu</a>

              <br>
              <em><font color="#663399"><strong>ACM MM, 2018</strong></font></em> &nbsp <font color="red"><strong>Oral Presentation!</strong></font>
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240628">Acm</a> /
              <a href="data/layout2019.bib">bibtex</a>
              <p></p>
              <p>We propose a novel Layout-Aware Convolutional Neural Network (LA-Net) for accurate monocular depth estimation by simultaneously perceiving scene layout and local depth details.</p>
            </td>
          </tr></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>CodeBase</heading>
              <p>
                <a href="https://github.com/qiuyu96/CoDeF">CoDeF</a>, <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiuyu96/CoDeF?style=social">
              </p>
              <p>
                <a href="https://github.com/JDAI-CV/fast-reid">FastReID: a Pytorch Toolbox for General Instance Re-identification</a>, <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/JDAI-CV/fast-reid?style=social">
              </p>
              <p>
                <a href="https://github.com/qiuyu96/Carver">Carver: Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase</a>,<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiuyu96/Carver?style=social">
              </p>
              <p>
                <a href="https://github.com/damo-vilab/Cones-V2">Cones 2</a>, <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/damo-vilab/Cones-V2?style=social">
              </p>
              <p>
                <a href="https://github.com/damo-vilab/Cones">Cones</a>, <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/damo-vilab/Cones?style=social">
              </p>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
            <td width="100%" valign="center">
              Journal reviewer: IJCV, TNNLS, TMM, YCSVT
              <br><br>
              Conference reviewer: CVPR, ICCV, ECCV, NeurIPS, ICML, ACM MM, AAAI

<!--              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, CVPR 2021</a>-->
<!--              <br><br>-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br><br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.

                <br>
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
